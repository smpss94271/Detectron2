{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"detectron2_balloon.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"a4AoqK-xiNeu","executionInfo":{"status":"ok","timestamp":1604839557329,"user_tz":-480,"elapsed":8990,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}},"outputId":"e0d3752c-2c2b-4289-c8b4-c89871160cde","colab":{"base_uri":"https://localhost:8080/"}},"source":["# 安裝套件\n","# install dependencies: \n","!pip install pyyaml==5.1 pycocotools>=2.0.1\n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())\n","!gcc --version\n","# opencv is pre-installed on colab"],"execution_count":1,"outputs":[{"output_type":"stream","text":["1.7.0+cu101 True\n","gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n","Copyright (C) 2017 Free Software Foundation, Inc.\n","This is free software; see the source for copying conditions.  There is NO\n","warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0u-GIRAyiZMp","executionInfo":{"status":"ok","timestamp":1604839573807,"user_tz":-480,"elapsed":9126,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}},"outputId":"33636665-3e23-42e2-8d33-3e0dd3117200","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 安裝 detectron2  pytorch 1.7 與 cuda10.1 版本\n","# install detectron2: (Colab has CUDA 10.1 + torch 1.6)\n","# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n","assert torch.__version__.startswith(\"1.7\")\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html\n","Collecting detectron2\n","\u001b[?25l  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/detectron2-0.3%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.6MB)\n","\u001b[K     |████████████████████████████████| 6.6MB 636kB/s \n","\u001b[?25hRequirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.3.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.3.0)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from detectron2) (2.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from detectron2) (3.2.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.16.0)\n","Collecting fvcore>=0.1.2\n","  Downloading https://files.pythonhosted.org/packages/e7/37/82dc217199c10288f3d05f50f342cb270ff2630841734bdfa40b54b0f8bc/fvcore-0.1.2.post20201104.tar.gz\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.8.7)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from detectron2) (2.3.0)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.1.0)\n","Collecting Pillow>=7.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/19/d4c25111d36163698396f93c363114cf1cddbacb24744f6612f25b6aa3d0/Pillow-8.0.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n","\u001b[K     |████████████████████████████████| 2.2MB 12.5MB/s \n","\u001b[?25hCollecting yacs>=0.1.6\n","  Downloading https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl\n","Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.6/dist-packages (from detectron2) (4.41.1)\n","Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot->detectron2) (2.4.7)\n","Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools>=2.0.2->detectron2) (50.3.2)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools>=2.0.2->detectron2) (0.29.21)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (1.3.1)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (1.18.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from fvcore>=0.1.2->detectron2) (5.1)\n","Collecting portalocker\n","  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.10.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.35.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (3.3.3)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.7.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.4.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (2.23.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.15.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.33.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.0.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (3.12.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.17.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (2.0.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.6)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.1.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (0.2.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2) (3.4.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard->detectron2) (0.4.8)\n","Building wheels for collected packages: fvcore\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.2.post20201104-cp36-none-any.whl size=44419 sha256=d8cc8eb575b67cefb8c4f508288acbd5c5d659f598164e17a07991a26d37239b\n","  Stored in directory: /root/.cache/pip/wheels/ec/4d/40/4077356fe02ef345791713eabede5ed63afe7d613b016694d1\n","Successfully built fvcore\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: yacs, portalocker, Pillow, fvcore, detectron2\n","  Found existing installation: Pillow 7.0.0\n","    Uninstalling Pillow-7.0.0:\n","      Successfully uninstalled Pillow-7.0.0\n","Successfully installed Pillow-8.0.1 detectron2-0.3+cu101 fvcore-0.1.2.post20201104 portalocker-2.0.0 yacs-0.1.8\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"7VCyn4y7ikYj","executionInfo":{"status":"ok","timestamp":1604839690760,"user_tz":-480,"elapsed":762,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}}},"source":["# 安ㄓㄨㄤ\n","import detectron2\n","import numpy as np\n","import json\n","import cv2\n","import random\n","import os\n","\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog\n","from detectron2.data import DatasetCatalog\n","from detectron2.engine import DefaultTrainer\n","from detectron2.structures import BoxMode\n","from detectron2.utils.visualizer import ColorMode\n","\n","from matplotlib import pyplot as plt\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"vzhVQ7gAipd-","executionInfo":{"status":"ok","timestamp":1604839694516,"user_tz":-480,"elapsed":845,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}},"outputId":"e74238cc-cba2-4514-e4d7-3f729a2f2766","colab":{"base_uri":"https://localhost:8080/"}},"source":["DatasetCatalog.list()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['coco_2014_train',\n"," 'coco_2014_val',\n"," 'coco_2014_minival',\n"," 'coco_2014_minival_100',\n"," 'coco_2014_valminusminival',\n"," 'coco_2017_train',\n"," 'coco_2017_val',\n"," 'coco_2017_test',\n"," 'coco_2017_test-dev',\n"," 'coco_2017_val_100',\n"," 'keypoints_coco_2014_train',\n"," 'keypoints_coco_2014_val',\n"," 'keypoints_coco_2014_minival',\n"," 'keypoints_coco_2014_valminusminival',\n"," 'keypoints_coco_2014_minival_100',\n"," 'keypoints_coco_2017_train',\n"," 'keypoints_coco_2017_val',\n"," 'keypoints_coco_2017_val_100',\n"," 'coco_2017_train_panoptic_separated',\n"," 'coco_2017_train_panoptic_stuffonly',\n"," 'coco_2017_train_panoptic',\n"," 'coco_2017_val_panoptic_separated',\n"," 'coco_2017_val_panoptic_stuffonly',\n"," 'coco_2017_val_panoptic',\n"," 'coco_2017_val_100_panoptic_separated',\n"," 'coco_2017_val_100_panoptic_stuffonly',\n"," 'coco_2017_val_100_panoptic',\n"," 'lvis_v1_train',\n"," 'lvis_v1_val',\n"," 'lvis_v1_test_dev',\n"," 'lvis_v1_test_challenge',\n"," 'lvis_v0.5_train',\n"," 'lvis_v0.5_val',\n"," 'lvis_v0.5_val_rand_100',\n"," 'lvis_v0.5_test',\n"," 'lvis_v0.5_train_cocofied',\n"," 'lvis_v0.5_val_cocofied',\n"," 'cityscapes_fine_instance_seg_train',\n"," 'cityscapes_fine_sem_seg_train',\n"," 'cityscapes_fine_instance_seg_val',\n"," 'cityscapes_fine_sem_seg_val',\n"," 'cityscapes_fine_instance_seg_test',\n"," 'cityscapes_fine_sem_seg_test',\n"," 'cityscapes_fine_panoptic_train',\n"," 'cityscapes_fine_panoptic_val',\n"," 'voc_2007_trainval',\n"," 'voc_2007_train',\n"," 'voc_2007_val',\n"," 'voc_2007_test',\n"," 'voc_2012_trainval',\n"," 'voc_2012_train',\n"," 'voc_2012_val',\n"," 'ade20k_sem_seg_train',\n"," 'ade20k_sem_seg_val']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"WG3V2yqDi8E4","executionInfo":{"status":"ok","timestamp":1604839699183,"user_tz":-480,"elapsed":938,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}},"outputId":"2f0392d1-7d02-4c19-ee86-1d50e32cd610","colab":{"base_uri":"https://localhost:8080/"}},"source":["MetadataCatalog.get('cityscapes_fine_instance_seg_train')\n"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Metadata(evaluator_type='cityscapes_instance', gt_dir='datasets/cityscapes/gtFine/train/', image_dir='datasets/cityscapes/leftImg8bit/train/', name='cityscapes_fine_instance_seg_train', stuff_classes=['road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light', 'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle'], thing_classes=['person', 'rider', 'car', 'truck', 'bus', 'train', 'motorcycle', 'bicycle'])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"2hg4q80bjAQ6","executionInfo":{"status":"ok","timestamp":1604839702244,"user_tz":-480,"elapsed":737,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}},"outputId":"fb14b2bb-31bc-4ee1-fc8f-9e84e4bb3585","colab":{"base_uri":"https://localhost:8080/"}},"source":["MetadataCatalog.get('keypoints_coco_2017_train').thing_classes"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['person']"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"p1uSvBZ_jDmJ","executionInfo":{"status":"ok","timestamp":1604839705427,"user_tz":-480,"elapsed":1820,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}},"outputId":"5a8da8ed-ae7c-440a-c0eb-3a9883c186b3","colab":{"base_uri":"https://localhost:8080/"}},"source":["!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n","!unzip balloon_dataset.zip > /dev/null"],"execution_count":6,"outputs":[{"output_type":"stream","text":["--2020-11-08 12:48:23--  https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n","Resolving github.com (github.com)... 140.82.112.3\n","Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/107595270/737339e2-2b83-11e8-856a-188034eb3468?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201108%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201108T124823Z&X-Amz-Expires=300&X-Amz-Signature=707657ce127f9678ae70f90f8e19ee4f1809f3f90b54bb9149a42ccef439a827&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=107595270&response-content-disposition=attachment%3B%20filename%3Dballoon_dataset.zip&response-content-type=application%2Foctet-stream [following]\n","--2020-11-08 12:48:23--  https://github-production-release-asset-2e65be.s3.amazonaws.com/107595270/737339e2-2b83-11e8-856a-188034eb3468?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20201108%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20201108T124823Z&X-Amz-Expires=300&X-Amz-Signature=707657ce127f9678ae70f90f8e19ee4f1809f3f90b54bb9149a42ccef439a827&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=107595270&response-content-disposition=attachment%3B%20filename%3Dballoon_dataset.zip&response-content-type=application%2Foctet-stream\n","Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.232.219\n","Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.232.219|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 38741381 (37M) [application/octet-stream]\n","Saving to: ‘balloon_dataset.zip’\n","\n","balloon_dataset.zip 100%[===================>]  36.95M  68.5MB/s    in 0.5s    \n","\n","2020-11-08 12:48:24 (68.5 MB/s) - ‘balloon_dataset.zip’ saved [38741381/38741381]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mnLxAmDijJ0g","executionInfo":{"status":"ok","timestamp":1604839708749,"user_tz":-480,"elapsed":717,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}}},"source":["def get_balloon_dicts(img_dir):\n","    json_file = os.path.join(img_dir, \"via_region_data.json\")\n","    with open(json_file) as f:\n","        imgs_anns = json.load(f)\n","\n","    dataset_dicts = []\n","    for idx, v in enumerate(imgs_anns.values()):\n","        record = {}\n","        \n","        filename = os.path.join(img_dir, v[\"filename\"])\n","        height, width = cv2.imread(filename).shape[:2]\n","        \n","        record[\"file_name\"] = filename\n","        record[\"image_id\"] = idx\n","        record[\"height\"] = height\n","        record[\"width\"] = width\n","      \n","        annos = v[\"regions\"]\n","        objs = []\n","        for _, anno in annos.items():\n","            assert not anno[\"region_attributes\"]\n","            anno = anno[\"shape_attributes\"]\n","            px = anno[\"all_points_x\"]\n","            py = anno[\"all_points_y\"]\n","            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n","            poly = [p for x in poly for p in x]\n","\n","            obj = {\n","                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n","                \"bbox_mode\": BoxMode.XYXY_ABS,\n","                \"segmentation\": [poly],\n","                \"category_id\": 0,\n","                \"iscrowd\": 0\n","            }\n","            objs.append(obj)\n","        record[\"annotations\"] = objs\n","        dataset_dicts.append(record)\n","    return dataset_dicts\n","\n","\n","for d in [\"train\", \"val\"]:\n","    DatasetCatalog.register(\"balloon_\" + d, lambda d=d: get_balloon_dicts(\"balloon/\" + d))\n","    MetadataCatalog.get(\"balloon_\" + d).set(thing_classes=[\"balloon\"])\n","balloon_metadata = MetadataCatalog.get(\"balloon_train\")\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWpNVvN1jMLd","executionInfo":{"status":"ok","timestamp":1604839713961,"user_tz":-480,"elapsed":1153,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}},"outputId":"3c3398ff-609c-401b-85f7-27bc9df09d09","colab":{"base_uri":"https://localhost:8080/"}},"source":["balloon_metadata\n"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Metadata(name='balloon_train', thing_classes=['balloon'])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"PIJOsn84jOsa","executionInfo":{"status":"ok","timestamp":1604839716142,"user_tz":-480,"elapsed":714,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}},"outputId":"e349f8b5-da56-46c2-a3fe-c82aaea7d763","colab":{"base_uri":"https://localhost:8080/"}},"source":["MetadataCatalog.get(\"balloon_val\")"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Metadata(name='balloon_val', thing_classes=['balloon'])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"D2GnTfACjRj0","executionInfo":{"status":"ok","timestamp":1604839718849,"user_tz":-480,"elapsed":791,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}},"outputId":"446e1933-b346-4a92-d9ba-4cbb5ae6c531","colab":{"base_uri":"https://localhost:8080/"}},"source":["DatasetCatalog.list()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['coco_2014_train',\n"," 'coco_2014_val',\n"," 'coco_2014_minival',\n"," 'coco_2014_minival_100',\n"," 'coco_2014_valminusminival',\n"," 'coco_2017_train',\n"," 'coco_2017_val',\n"," 'coco_2017_test',\n"," 'coco_2017_test-dev',\n"," 'coco_2017_val_100',\n"," 'keypoints_coco_2014_train',\n"," 'keypoints_coco_2014_val',\n"," 'keypoints_coco_2014_minival',\n"," 'keypoints_coco_2014_valminusminival',\n"," 'keypoints_coco_2014_minival_100',\n"," 'keypoints_coco_2017_train',\n"," 'keypoints_coco_2017_val',\n"," 'keypoints_coco_2017_val_100',\n"," 'coco_2017_train_panoptic_separated',\n"," 'coco_2017_train_panoptic_stuffonly',\n"," 'coco_2017_train_panoptic',\n"," 'coco_2017_val_panoptic_separated',\n"," 'coco_2017_val_panoptic_stuffonly',\n"," 'coco_2017_val_panoptic',\n"," 'coco_2017_val_100_panoptic_separated',\n"," 'coco_2017_val_100_panoptic_stuffonly',\n"," 'coco_2017_val_100_panoptic',\n"," 'lvis_v1_train',\n"," 'lvis_v1_val',\n"," 'lvis_v1_test_dev',\n"," 'lvis_v1_test_challenge',\n"," 'lvis_v0.5_train',\n"," 'lvis_v0.5_val',\n"," 'lvis_v0.5_val_rand_100',\n"," 'lvis_v0.5_test',\n"," 'lvis_v0.5_train_cocofied',\n"," 'lvis_v0.5_val_cocofied',\n"," 'cityscapes_fine_instance_seg_train',\n"," 'cityscapes_fine_sem_seg_train',\n"," 'cityscapes_fine_instance_seg_val',\n"," 'cityscapes_fine_sem_seg_val',\n"," 'cityscapes_fine_instance_seg_test',\n"," 'cityscapes_fine_sem_seg_test',\n"," 'cityscapes_fine_panoptic_train',\n"," 'cityscapes_fine_panoptic_val',\n"," 'voc_2007_trainval',\n"," 'voc_2007_train',\n"," 'voc_2007_val',\n"," 'voc_2007_test',\n"," 'voc_2012_trainval',\n"," 'voc_2012_train',\n"," 'voc_2012_val',\n"," 'ade20k_sem_seg_train',\n"," 'ade20k_sem_seg_val',\n"," 'balloon_train',\n"," 'balloon_val']"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"Nms9qZP9jU99","executionInfo":{"status":"ok","timestamp":1604839725454,"user_tz":-480,"elapsed":2447,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}},"outputId":"3f1c7ea2-c0d7-4752-b3e6-68a02c4b17cc","colab":{"base_uri":"https://localhost:8080/"}},"source":["DatasetCatalog.get('balloon_train')[0]"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'annotations': [{'bbox': [994, 619, 1445, 1166],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 0,\n","   'iscrowd': 0,\n","   'segmentation': [[1020.5,\n","     963.5,\n","     1000.5,\n","     899.5,\n","     994.5,\n","     841.5,\n","     1003.5,\n","     787.5,\n","     1023.5,\n","     738.5,\n","     1050.5,\n","     700.5,\n","     1089.5,\n","     663.5,\n","     1134.5,\n","     638.5,\n","     1190.5,\n","     621.5,\n","     1265.5,\n","     619.5,\n","     1321.5,\n","     643.5,\n","     1361.5,\n","     672.5,\n","     1403.5,\n","     720.5,\n","     1428.5,\n","     765.5,\n","     1442.5,\n","     800.5,\n","     1445.5,\n","     860.5,\n","     1441.5,\n","     896.5,\n","     1427.5,\n","     942.5,\n","     1400.5,\n","     990.5,\n","     1361.5,\n","     1035.5,\n","     1316.5,\n","     1079.5,\n","     1269.5,\n","     1112.5,\n","     1228.5,\n","     1129.5,\n","     1198.5,\n","     1134.5,\n","     1207.5,\n","     1144.5,\n","     1210.5,\n","     1153.5,\n","     1190.5,\n","     1166.5,\n","     1177.5,\n","     1166.5,\n","     1172.5,\n","     1150.5,\n","     1174.5,\n","     1136.5,\n","     1170.5,\n","     1129.5,\n","     1153.5,\n","     1122.5,\n","     1127.5,\n","     1112.5,\n","     1104.5,\n","     1084.5,\n","     1061.5,\n","     1037.5,\n","     1032.5,\n","     989.5,\n","     1020.5,\n","     963.5]]}],\n"," 'file_name': 'balloon/train/34020010494_e5cb88e1c4_k.jpg',\n"," 'height': 1536,\n"," 'image_id': 0,\n"," 'width': 2048}"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"hA4n3A00jYVd","executionInfo":{"status":"ok","timestamp":1604839730855,"user_tz":-480,"elapsed":2339,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}},"outputId":"18b8a1c7-e1d1-4523-9b66-009dd9b14063","colab":{"base_uri":"https://localhost:8080/"}},"source":["# dataset_dicts = get_balloon_dicts(\"balloon/train\")\n","\n","dataset_dicts = DatasetCatalog.get('balloon_train')\n","print(len(dataset_dicts))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["61\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4MqCoDmqjbX7","executionInfo":{"status":"ok","timestamp":1604839736267,"user_tz":-480,"elapsed":3999,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}},"outputId":"4828b859-6a0b-4aa4-fdc6-2e4f61781c12","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1x1P3TB0hk4ffD24y84BWFsCAZvQ8eWib"}},"source":["# 辨識氣球\n","for d in random.sample(dataset_dicts, 3):\n","    img = cv2.imread(d[\"file_name\"])\n","    visualizer = Visualizer(img[:, :, ::-1], metadata=balloon_metadata, scale=0.5)\n","    vis = visualizer.draw_dataset_dict(d)\n","    plt.figure(figsize=(20,10))\n","    plt.imshow(vis.get_image())"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"XslKc6pBjgs0","executionInfo":{"status":"ok","timestamp":1604839927322,"user_tz":-480,"elapsed":182458,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}},"outputId":"588bb06a-6049-4d1c-edaf-fc15ff3f4d98","colab":{"base_uri":"https://localhost:8080/"}},"source":["cfg = get_cfg()\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n","cfg.DATASETS.TRAIN = (\"balloon_train\",)\n","# cfg.DATASETS.TEST = (\"balloon_val\")\n","cfg.DATASETS.TEST = ()\n","cfg.DATALOADER.NUM_WORKERS = 2\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n","cfg.SOLVER.IMS_PER_BATCH = 2\n","cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n","cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you may need to train longer for a practical dataset\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon)\n","\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","trainer = DefaultTrainer(cfg) \n","trainer.resume_or_load(resume=False)\n","trainer.train()\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["\u001b[32m[11/08 12:49:16 d2.engine.defaults]: \u001b[0mModel:\n","GeneralizedRCNN(\n","  (backbone): FPN(\n","    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (top_block): LastLevelMaxPool()\n","    (bottom_up): ResNet(\n","      (stem): BasicStem(\n","        (conv1): Conv2d(\n","          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","      )\n","      (res2): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res3): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res4): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (4): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (5): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res5): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (proposal_generator): RPN(\n","    (rpn_head): StandardRPNHead(\n","      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (anchor_generator): DefaultAnchorGenerator(\n","      (cell_anchors): BufferList()\n","    )\n","  )\n","  (roi_heads): StandardROIHeads(\n","    (box_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (box_head): FastRCNNConvFCHead(\n","      (flatten): Flatten(start_dim=1, end_dim=-1)\n","      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc_relu1): ReLU()\n","      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n","      (fc_relu2): ReLU()\n","    )\n","    (box_predictor): FastRCNNOutputLayers(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n","    )\n","    (mask_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (mask_head): MaskRCNNConvUpsampleHead(\n","      (mask_fcn1): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn2): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn3): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn4): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (deconv_relu): ReLU()\n","      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")\n","\u001b[32m[11/08 12:49:17 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 61 images left.\n","\u001b[32m[11/08 12:49:17 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n","\u001b[36m|  category  | #instances   |\n","|:----------:|:-------------|\n","|  balloon   | 255          |\n","|            |              |\u001b[0m\n","\u001b[32m[11/08 12:49:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[32m[11/08 12:49:17 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n","\u001b[32m[11/08 12:49:17 d2.data.common]: \u001b[0mSerializing 61 elements to byte tensors and concatenating them all ...\n","\u001b[32m[11/08 12:49:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.17 MiB\n"],"name":"stdout"},{"output_type":"stream","text":["model_final_f10217.pkl: 178MB [00:21, 8.13MB/s]                           \n","Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[11/08 12:49:45 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/detectron2/structures/masks.py:345: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  item = item.nonzero().squeeze(1).cpu().numpy().tolist()\n","/usr/local/lib/python3.6/dist-packages/detectron2/structures/masks.py:345: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  item = item.nonzero().squeeze(1).cpu().numpy().tolist()\n","/usr/local/lib/python3.6/dist-packages/detectron2/modeling/roi_heads/fast_rcnn.py:217: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  num_fg = fg_inds.nonzero().numel()\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[11/08 12:49:54 d2.utils.events]: \u001b[0m eta: 0:01:58  iter: 19  total_loss: 1.959  loss_cls: 0.6801  loss_box_reg: 0.5532  loss_mask: 0.6907  loss_rpn_cls: 0.01953  loss_rpn_loc: 0.007498  time: 0.4302  data_time: 0.0282  lr: 4.9953e-06  max_mem: 2672M\n","\u001b[32m[11/08 12:50:02 d2.utils.events]: \u001b[0m eta: 0:01:51  iter: 39  total_loss: 2.019  loss_cls: 0.6431  loss_box_reg: 0.6245  loss_mask: 0.6656  loss_rpn_cls: 0.04311  loss_rpn_loc: 0.009571  time: 0.4329  data_time: 0.0143  lr: 9.9902e-06  max_mem: 2672M\n","\u001b[32m[11/08 12:50:11 d2.utils.events]: \u001b[0m eta: 0:01:43  iter: 59  total_loss: 1.957  loss_cls: 0.6003  loss_box_reg: 0.709  loss_mask: 0.6077  loss_rpn_cls: 0.01666  loss_rpn_loc: 0.0106  time: 0.4324  data_time: 0.0064  lr: 1.4985e-05  max_mem: 2672M\n","\u001b[32m[11/08 12:50:20 d2.utils.events]: \u001b[0m eta: 0:01:35  iter: 79  total_loss: 1.795  loss_cls: 0.5166  loss_box_reg: 0.6762  loss_mask: 0.5324  loss_rpn_cls: 0.03556  loss_rpn_loc: 0.01018  time: 0.4341  data_time: 0.0073  lr: 1.998e-05  max_mem: 2672M\n","\u001b[32m[11/08 12:50:29 d2.utils.events]: \u001b[0m eta: 0:01:28  iter: 99  total_loss: 1.711  loss_cls: 0.4648  loss_box_reg: 0.684  loss_mask: 0.4743  loss_rpn_cls: 0.029  loss_rpn_loc: 0.005555  time: 0.4389  data_time: 0.0068  lr: 2.4975e-05  max_mem: 2672M\n","\u001b[32m[11/08 12:50:38 d2.utils.events]: \u001b[0m eta: 0:01:19  iter: 119  total_loss: 1.582  loss_cls: 0.4122  loss_box_reg: 0.6813  loss_mask: 0.4182  loss_rpn_cls: 0.0231  loss_rpn_loc: 0.004701  time: 0.4451  data_time: 0.0065  lr: 2.997e-05  max_mem: 2672M\n","\u001b[32m[11/08 12:50:48 d2.utils.events]: \u001b[0m eta: 0:01:11  iter: 139  total_loss: 1.336  loss_cls: 0.3614  loss_box_reg: 0.59  loss_mask: 0.3373  loss_rpn_cls: 0.02426  loss_rpn_loc: 0.004039  time: 0.4471  data_time: 0.0081  lr: 3.4965e-05  max_mem: 2672M\n","\u001b[32m[11/08 12:50:57 d2.utils.events]: \u001b[0m eta: 0:01:02  iter: 159  total_loss: 1.401  loss_cls: 0.3459  loss_box_reg: 0.7118  loss_mask: 0.3329  loss_rpn_cls: 0.03585  loss_rpn_loc: 0.008578  time: 0.4486  data_time: 0.0058  lr: 3.996e-05  max_mem: 2672M\n","\u001b[32m[11/08 12:51:06 d2.utils.events]: \u001b[0m eta: 0:00:54  iter: 179  total_loss: 1.276  loss_cls: 0.2893  loss_box_reg: 0.618  loss_mask: 0.2708  loss_rpn_cls: 0.02127  loss_rpn_loc: 0.006296  time: 0.4499  data_time: 0.0093  lr: 4.4955e-05  max_mem: 2672M\n","\u001b[32m[11/08 12:51:16 d2.utils.events]: \u001b[0m eta: 0:00:45  iter: 199  total_loss: 1.249  loss_cls: 0.2753  loss_box_reg: 0.6491  loss_mask: 0.2584  loss_rpn_cls: 0.01886  loss_rpn_loc: 0.009132  time: 0.4527  data_time: 0.0071  lr: 4.995e-05  max_mem: 2672M\n","\u001b[32m[11/08 12:51:25 d2.utils.events]: \u001b[0m eta: 0:00:36  iter: 219  total_loss: 1.038  loss_cls: 0.2244  loss_box_reg: 0.6001  loss_mask: 0.1732  loss_rpn_cls: 0.01263  loss_rpn_loc: 0.005063  time: 0.4544  data_time: 0.0061  lr: 5.4945e-05  max_mem: 2672M\n","\u001b[32m[11/08 12:51:35 d2.utils.events]: \u001b[0m eta: 0:00:27  iter: 239  total_loss: 1.111  loss_cls: 0.2084  loss_box_reg: 0.6214  loss_mask: 0.2133  loss_rpn_cls: 0.02063  loss_rpn_loc: 0.00638  time: 0.4568  data_time: 0.0089  lr: 5.994e-05  max_mem: 2672M\n","\u001b[32m[11/08 12:51:44 d2.utils.events]: \u001b[0m eta: 0:00:18  iter: 259  total_loss: 0.9757  loss_cls: 0.1798  loss_box_reg: 0.6226  loss_mask: 0.1825  loss_rpn_cls: 0.0136  loss_rpn_loc: 0.004526  time: 0.4582  data_time: 0.0081  lr: 6.4935e-05  max_mem: 2672M\n","\u001b[32m[11/08 12:51:54 d2.utils.events]: \u001b[0m eta: 0:00:09  iter: 279  total_loss: 0.89  loss_cls: 0.1629  loss_box_reg: 0.5241  loss_mask: 0.149  loss_rpn_cls: 0.02123  loss_rpn_loc: 0.01177  time: 0.4605  data_time: 0.0058  lr: 6.993e-05  max_mem: 2672M\n","\u001b[32m[11/08 12:52:05 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 299  total_loss: 0.7745  loss_cls: 0.1424  loss_box_reg: 0.4892  loss_mask: 0.1425  loss_rpn_cls: 0.02303  loss_rpn_loc: 0.008863  time: 0.4631  data_time: 0.0077  lr: 7.4925e-05  max_mem: 2672M\n","\u001b[32m[11/08 12:52:06 d2.engine.hooks]: \u001b[0mOverall training speed: 298 iterations in 0:02:18 (0.4631 s / it)\n","\u001b[32m[11/08 12:52:06 d2.engine.hooks]: \u001b[0mTotal training time: 0:02:20 (0:00:02 on hooks)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sLeC9_IojoWO","executionInfo":{"status":"ok","timestamp":1604840083450,"user_tz":-480,"elapsed":1831,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}}},"source":["cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n","cfg.DATASETS.TEST = (\"balloon_val\", )\n","predictor = DefaultPredictor(cfg)\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"lb7eozNNj01_","executionInfo":{"status":"ok","timestamp":1604840090816,"user_tz":-480,"elapsed":5352,"user":{"displayName":"彭鈺琇","photoUrl":"","userId":"15855200896632468651"}},"outputId":"ce7024ab-6253-44ec-d21c-34a8a835cbcd","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"16c3yLuQ_acT0ZnzS0xEr21xSXAUoQ3g3"}},"source":["# 辨識氣球\n","val_dicts = DatasetCatalog.get('balloon_val')\n","for d in random.sample(val_dicts, 3):    \n","    im = cv2.imread(d[\"file_name\"])\n","    outputs = predictor(im)\n","    v = Visualizer(im[:, :, ::-1],\n","                   metadata=balloon_metadata, \n","                   scale=0.8, \n","                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels\n","    )\n","    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","    plt.figure(figsize=(20,10))\n","    plt.imshow(v.get_image()[:,:,::-1])\n","    "],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}